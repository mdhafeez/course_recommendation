{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 1s 4ms/step - loss: 1.4600 - accuracy: 0.3333\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3243 - accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2056 - accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1145 - accuracy: 0.3417\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0320 - accuracy: 0.3917\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9688 - accuracy: 0.5667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9179 - accuracy: 0.6583\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8712 - accuracy: 0.6750\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8310 - accuracy: 0.6750\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7977 - accuracy: 0.6750\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7665 - accuracy: 0.6750\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7366 - accuracy: 0.6750\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7141 - accuracy: 0.6750\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.6750\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.6750\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.7917\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.8917\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.8750\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.8500\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.8667\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5994 - accuracy: 0.8500\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5900 - accuracy: 0.8583\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5821 - accuracy: 0.8583\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5740 - accuracy: 0.8583\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.8667\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.8667\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.8500\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5470 - accuracy: 0.8500\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.8500\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.8500\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.8583\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.8583\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.8583\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.8667\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.8667\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.8667\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.8667\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.8583\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.8583\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.8583\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.8667\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.8667\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.8667\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.8667\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.8667\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.8667\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.8833\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4531 - accuracy: 0.8833\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.8833\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.4507 - accuracy: 0.8333\n",
      "Test accuracy: 0.8333333134651184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris_data = load_iris()\n",
    "\n",
    "# Features (Sepal Length, Sepal Width, Petal Length, Petal Width)\n",
    "X = iris_data['data']\n",
    "\n",
    "# Labels (Species: Setosa, Versicolor, Virginica)\n",
    "y = iris_data['target']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=(4,), activation='relu'),  # Hidden layer with 10 neurons\n",
    "    keras.layers.Dense(3, activation='softmax')  # Output layer with 3 neurons (3 species)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', # adjust the weight by optimizer\n",
    "              loss='sparse_categorical_crossentropy', # variation \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50) #training going to 50 times\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to all_course_links.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = 'https://www.dtims.intan.my/search_course?page='\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 10  # Adjust this number based on the total pages you want to scrape\n",
    "\n",
    "# List to store course data\n",
    "course_data = []\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(1, num_pages + 1):\n",
    "    # Construct the full URL for the current page\n",
    "    url = base_url + str(page)\n",
    "    \n",
    "    # Send a request to the website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all the course links\n",
    "    course_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Extract course codes and URLs from each page\n",
    "    for link in course_links:\n",
    "        if 'detail_course' in link['href']:\n",
    "            course_url = link['href']\n",
    "            # Extracting course code from the URL (last part of the URL)\n",
    "            course_code = course_url.split('/')[-1]\n",
    "            course_data.append({'course_code': course_code, 'course_url': course_url})\n",
    "\n",
    "# Saving the data into a CSV file\n",
    "csv_filename = 'all_course_links.csv'\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['course_code', 'course_url'])\n",
    "    writer.writeheader()\n",
    "    for course in course_data:\n",
    "        writer.writerow(course)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "The predicted species for the input [[5.9, 3.0, 5.1, 1.8]] is: Virginica\n"
     ]
    }
   ],
   "source": [
    "# Example new input (sepal length, sepal width, petal length, petal width)\n",
    "new_input = [[5.9, 3.0, 5.1, 1.8]]  # Replace this with your own measurements\n",
    "\n",
    "# Use the trained model to predict the species of the new input\n",
    "prediction = model.predict(new_input)\n",
    "\n",
    "# Get the predicted class (species) with the highest probability\n",
    "predicted_class = tf.argmax(prediction[0]).numpy()\n",
    "\n",
    "# Mapping predicted class back to species name\n",
    "species_mapping = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
    "predicted_species = species_mapping[predicted_class]\n",
    "\n",
    "# Print the predicted species\n",
    "print(f'The predicted species for the input {new_input} is: {predicted_species}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAADeCAYAAADLhdi2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAd40lEQVR4nO3de7iWVZk/8HtzDHVUEEYULw5tFcsC1DxbUKBZMYqHYhgNTxc6g6dplLExPAUomTYjjo7HNh64LtDwgE4NVuChRIPMZsQwrDygQIiBgCiSz+8Pf5LEs1544V1s9t6fz3Xxh9+17+dZe7sX77553r1WXVEURQAAAABZtGrsCQAAAEBzpvEGAACAjDTeAAAAkJHGGwAAADLSeAMAAEBGGm8AAADISOMNAAAAGWm8AQAAICONNwAAAGSk8S4xceLEqKurizlz5tTkenV1dXHOOefU5Fofvebll1++WbUvvfRS1NXVlf6ZPHlyTedJ89Dc10RExHvvvRdXXHFF9OzZM9q3bx/77LNPXH/99bWbIM1KS1gTH/WTn/xk3evEG2+8UZNr0ry0hDUxevToGDx4cHTr1i3q6uri1FNPrdncaH5awpr47W9/GyeccEJ07Ngxtttuuzj44INj2rRptZtgM6PxbsHOPffcmDVr1np/jjzyyMaeFjSKkSNHxlVXXRVnn312TJ8+PY477rg4//zz48orr2zsqUGjWrlyZYwYMSJ23333xp4KNKp///d/j6VLl8YxxxwT7dq1a+zpQKN66aWX4tBDD40XXnghbrrpprj33nujS5cuMWTIkJg6dWpjT2+b1KaxJ0Dj6d69exxyyCGNPQ1odHPnzo3bb789xo0bF6NGjYqIiAEDBsTSpUtj7Nix8Y//+I/RqVOnRp4lNI5vfvOb0bFjx/jKV74SY8eObezpQKNZsWJFtGr1wTOru+66q5FnA41r/Pjx8fbbb8f06dOjW7duERFx9NFHx6c//en4xje+Eccdd9y69cIHfDU20zvvvBMXXHBB9OvXL3baaafo1KlTHHroofHggw8ma26++ebYe++9o3379vHJT36y9G3dixYtirPOOiv22GOPaNeuXfTq1SuuuOKKWLt2bc5PB7ZYU14TDzzwQBRFEaeddtp6+WmnnRarV6+O//mf/6nZvWg5mvKa+NATTzwRt9xyS9x2223RunXrml+flqWprwlNBLXWlNfEz3/+8+jbt++6pjsionXr1vGlL30pXn311fjFL35Rs3s1F554b6Z333033nzzzbjwwgujW7dusWbNmvjJT34Sxx9/fDQ0NMTw4cPX+/hp06bFzJkz49vf/nZsv/32ceONN8awYcOiTZs2ceKJJ0bEB4vkoIMOilatWsWll14a9fX1MWvWrBg7dmy89NJL0dDQUHFOPXv2jIgP3vqxKcaPHx8XX3xxtGnTJvbff//413/91zjmmGOq/lpARNNeE88991x06dIlunbtul7ep0+fdeNQraa8JiIiVq9eHWeccUb88z//c+y///5+b48t1tTXBNRaU14Ta9asKX03YPv27SMi4n//93+9s/avFWygoaGhiIhi9uzZm1yzdu3a4r333ivOOOOMYr/99ltvLCKKDh06FIsWLVrv4/fZZ59izz33XJedddZZxQ477FC8/PLL69Vfc801RUQUc+fOXe+al1122XofV19fX9TX1290rq+//noxYsSI4p577imeeOKJYtKkScUhhxxSRERx6623bvLnTMvR3NfEkUceWfTu3bt0rF27dsWZZ5650WvQsjT3NVEURXHBBRcUH//4x4u33367KIqiuOyyy4qIKJYsWbJJ9bQsLWFNfNT2229fnHLKKVXX0XI09zUxZMiQYueddy5WrFixXv7Zz362iIjiyiuv3Og1WhrvmdkC9957bxx++OGxww47RJs2baJt27Zx++23x29+85sNPnbgwIGx6667rvvv1q1bx9ChQ+PFF1+MBQsWRETEww8/HJ///Odj9913j7Vr167786UvfSkiIh577LGK83nxxRfjxRdf3Oi8d9ttt7jlllviq1/9ahxxxBHxD//wD/H444/HfvvtF9/85je9rZ3N1lTXRMQHO3tuzhhU0lTXxC9+8Yv4j//4j7j55pujQ4cO1XzKUFFTXROQS1NdE+ecc04sX748hg8fHr///e9j8eLFcckll8STTz4ZEX41o4yvyGa677774mtf+1p069Yt7r777pg1a1bMnj07Tj/99HjnnXc2+Pi/fgvrR7OlS5dGRMTixYvjoYceirZt2673Z999942IyHqES9u2bWPo0KGxdOnSmD9/frb70Hw15TWxyy67rLvnR61atSr5VirYmKa8Jk4//fQ4/vjj4zOf+UwsW7Ysli1btm7Ob731VqxYsaIm96FlacprAnJoymti4MCB0dDQEI8//njU19dH165d47777osxY8ZERKz3u998wO94b6a77747evXqFVOmTFnvadi7775b+vGLFi1KZrvssktERHTu3Dn69OkT48aNK71G7qNciqKICP9CxeZpymvi05/+dEyePDkWLVq03ova//3f/0VExKc+9ama3IeWpSmviblz58bcuXPj3nvv3WCsvr4++vbtG88++2xN7kXL0ZTXBOTQ1NfEKaecEieddFLMnz8/2rZtG3vuuWdcddVVUVdXF5/97Gdrdp/mQuO9merq6qJdu3brLZJFixYldyH86U9/GosXL1739pA///nPMWXKlKivr4899tgjIiIGDx4cP/zhD6O+vj46duyY/5P4iPfeey+mTJkSnTt3jj333HOr3pvmoSmviWOPPTZGjx4dd9xxR1x00UXr8okTJ0aHDh3i6KOPznZvmq+mvCZmzpy5QTZx4sS444474oEHHvAkg83SlNcE5NAc1kSbNm3iE5/4RERELF++PG655ZY49thjo0ePHtnv3dRovCuYMWNG6Y5+X/7yl2Pw4MFx3333xciRI+PEE0+MV199NcaMGRO77bZb6Vu1O3fuHF/4whfikksuWbcL4bx589Y7AuDb3/52/PjHP47DDjsszjvvvOjdu3e888478dJLL8UPf/jDuOmmm9YtqjIfNswb+72Mf/mXf4n33nsvDj/88OjatWu8+uqrcf3118ezzz4bDQ0NjowhqbmuiX333TfOOOOMuOyyy6J169Zx4IEHxiOPPBK33HJLjB071lvNSWqua2LAgAEbZI8++mhERBx++OHRuXPnivW0XM11TUR88LuxS5YsiYgPGp6XX345fvCDH0RERP/+/aNLly4bvQYtT3NdE3/84x/j2muvjcMPPzz+5m/+JubNmxdXX311tGrVKm644YZN/Oq0MI29u9u26MNdCFN//vCHPxRFURTjx48vevbsWbRv3774xCc+Udx6663rdn39qIgozj777OLGG28s6uvri7Zt2xb77LNPMWnSpA3uvWTJkuK8884revXqVbRt27bo1KlTccABBxTf+ta3ipUrV653zb/ehbBHjx5Fjx49Nvr53X777cVBBx1UdOrUqWjTpk3RsWPH4otf/GIxffr0qr9WtAzNfU0URVGsWbOmuOyyy4ru3bsX7dq1K/bee+9iwoQJVX2daDlawpr4a3Y1p5KWsCb69++f/PxmzpxZzZeLFqC5r4mlS5cWRx11VNGlS5eibdu2Rffu3Ytzzz3Xa0QFdUXx/3+xFwAAAKg5u2gBAABARhpvAAAAyEjjDQAAABlpvAEAACAjjTcAAABkpPEGAACAjDTeAAAAkFGbTf3Aurq6nPOARrElx9hbEzRH1gRsaHPXhTVBc+R1Aja0KevCE28AAADISOMNAAAAGWm8AQAAICONNwAAAGSk8QYAAICMNN4AAACQkcYbAAAAMtJ4AwAAQEYabwAAAMhI4w0AAAAZabwBAAAgI403AAAAZKTxBgAAgIw03gAAAJCRxhsAAAAy0ngDAABARhpvAAAAyEjjDQAAABlpvAEAACAjjTcAAABkpPEGAACAjDTeAAAAkJHGGwAAADLSeAMAAEBGGm8AAADIqE1jTwBgcx1wwAHJsXPOOac0Hz58eLLmzjvvLM2vv/76ZM0zzzyTHAMAgAhPvAEAACArjTcAAABkpPEGAACAjDTeAAAAkJHGGwAAADKqK4qi2KQPrKvLPZdmqXXr1qX5TjvtVNP7pHZw3m677ZI1vXv3Ls3PPvvsZM0111xTmg8bNixZ884775Tm48ePT9ZcccUVybFa2sRv/1LWxNbTr1+/0nzGjBnJmh133LFm91++fHlybJdddqnZfbYF1gS1MHDgwNJ80qRJyZr+/fuX5i+88EJN5rQlNnddWBPNz+jRo0vzSj+3tGpV/pxrwIAByZrHHnusqnltTV4nYEObsi488QYAAICMNN4AAACQkcYbAAAAMtJ4AwAAQEYabwAAAMhI4w0AAAAZtWnsCTSW7t27l+bt2rVL1hx22GGl+RFHHJGs2XnnnUvzE044IT25rWTBggWl+YQJE5I1xx13XGm+YsWKZM2vf/3r0nxbPiqDre+ggw5Kjk2dOrU0r3QsX+pYh0rfq2vWrCnNKx0Zdsghh5TmzzzzTNX3YdN87nOfS46l/l/df//9uaZDiQMPPLA0nz179laeCVTv1FNPTY5ddNFFpfn7779f9X225FguoOnxxBsAAAAy0ngDAABARhpvAAAAyEjjDQAAABlpvAEAACCjZr2reb9+/ZJjM2bMKM0r7ZLcFFXaZXP06NGl+cqVK5M1kyZNKs0XLlyYrPnTn/5Umr/wwgvJGpq27bbbLjm2//77l+Z33313sma33Xbb4jl9aP78+cmxq6++ujSfPHlysubnP/95aZ5aXxERV111VXKMjRswYEBybK+99irN7Wpee61apf/tvlevXqV5jx49kjV1dXVbPCeohUrfpx/72Me24kzgAwcffHBy7OSTTy7N+/fvn6zZd999q57DhRdemBx7/fXXS/NKJz+lfu57+umnq5tYE+KJNwAAAGSk8QYAAICMNN4AAACQkcYbAAAAMtJ4AwAAQEYabwAAAMioWR8n9sorryTHli5dWppvC8eJpbbRX7ZsWbLm85//fGm+Zs2aZM1dd91V1bxgU918883JsWHDhm3FmWwodZxZRMQOO+xQmj/22GPJmtTRVn369KlqXmy64cOHJ8dmzZq1FWfSslU65m/EiBGleaVjA+fNm7fFc4JqDBo0qDQ/99xzq75Wpe/fwYMHl+aLFy+u+j40b0OHDi3Nr7vuumRN586dS/NKRzQ++uijybEuXbqU5t/97neTNSmV5pC6z9///d9XfZ+mwhNvAAAAyEjjDQAAABlpvAEAACAjjTcAAABkpPEGAACAjJr1ruZvvvlmcmzUqFGleWrnyYiIX/3qV6X5hAkTqptYRDz77LPJsSOPPLI0X7VqVbJm3333Lc3PP//8quYF1TjggANK86985SvJmko7XKakdhV/6KGHkjXXXHNNaf76668na1Jr/E9/+lOy5gtf+EJpvjmfJ5umVSv/ZrwtuO2226qumT9/foaZQNoRRxyRHGtoaCjNN+eEm0o7Pr/88stVX4+mr02b8jbrM5/5TLLm1ltvLc232267ZM3jjz9emo8ZMyZZ87Of/Sw51r59+9L8nnvuSdYcddRRybGUOXPmVF3T1PnpBQAAADLSeAMAAEBGGm8AAADISOMNAAAAGWm8AQAAICONNwAAAGTUrI8Tq+SBBx4ozWfMmJGsWbFiRWnet2/fZM0ZZ5xRmqeOOoqofGxYyty5c0vzM888s+prwUf169cvOfbjH/+4NN9xxx2TNUVRlOY/+tGPkjXDhg0rzfv375+sGT16dGle6QikJUuWlOa//vWvkzXvv/9+aV7pSLX999+/NH/mmWeSNS1Rnz59SvNdd911K8+EMptz5FLq7wzI5ZRTTkmO7b777lVf79FHHy3N77zzzqqvRfN28sknl+abcxRjpb87hw4dWpq/9dZbVd+n0vU258iwBQsWJMfuuOOOqq/X1HniDQAAABlpvAEAACAjjTcAAABkpPEGAACAjDTeAAAAkFGL3dU8ZXN2AFy+fHnVNSNGjEiOTZkypTRP7Z4MtbD33nuX5qNGjUrWpHY1fuONN5I1CxcuLM0r7W65cuXK0vy///u/kzWVxraGDh06JMcuuOCC0vykk07KNZ0m6ctf/nJpXulrS+2ldpHv1atX1dd67bXXtnQ6UKpz586l+emnn56sSf1ctWzZsmTN2LFjq5oXzduYMWOSYxdffHFpnjrdJSLixhtvLM1TJ7VEbP7u5Snf+ta3anat8847LzmWOkmmOfPEGwAAADLSeAMAAEBGGm8AAADISOMNAAAAGWm8AQAAICONNwAAAGTkOLEauPzyy5NjBxxwQGnev3//ZM2gQYNK80ceeaSqecFfa9++fXLsmmuuKc1TRzpFRKxYsaI0Hz58eLJmzpw5pXlLOiKqe/fujT2FJqF3795V18ydOzfDTFq21N8NqWPGIiJ++9vfluapvzNgU/Ts2TM5NnXq1Jrd5/rrr0+OzZw5s2b3oem49NJLS/PUkWEREWvWrCnNp0+fnqy56KKLSvPVq1dXmF25j33sY8mxo446KjmW+hmlrq4uWZM6Zu/BBx9M1rREnngDAABARhpvAAAAyEjjDQAAABlpvAEAACAjjTcAAABkZFfzGli1alVybMSIEaX5M888k6y59dZbS/NKO2mmdoq+4YYbkjVFUSTHaJ7222+/5Fil3ctTjj322NL8scceq/paUAuzZ89u7Ck0uh133DE5dvTRR5fmJ598crKm0u63KWPGjCnNly1bVvW14EOp79+IiD59+lR9vZ/+9Kel+XXXXVf1tWj6dt555+TYyJEjS/NKP0undi8fMmRINdPaqD333LM0nzRpUrImdepSJT/4wQ+SY1dffXXV12uJPPEGAACAjDTeAAAAkJHGGwAAADLSeAMAAEBGGm8AAADISOMNAAAAGTlOLLPf/e53pfmpp56arGloaCjNv/71rydrUmPbb799subOO+8szRcuXJisoWn73ve+lxyrq6srzSsdDebYsIhWrcr//fL999/fyjMhIqJTp05b5T59+/YtzVPrKCJi0KBBpfkee+yRrGnXrl1pftJJJyVrUt+TERGrV68uzZ9++ulkzbvvvluat2mT/hHil7/8ZXIMNiZ13NL48eOrvtbPfvaz5Ngpp5xSmi9fvrzq+9D0pf6+jYjo3Llz1dc777zzSvO//du/TdacdtpppfkxxxyTrPnUpz5Vmu+www7JmkrHoKXG7r777mRNpaOV+QtPvAEAACAjjTcAAABkpPEGAACAjDTeAAAAkJHGGwAAADKyq3kjuf/++5Nj8+fPL80r7Ug9cODA0vzKK69M1vTo0aM0HzduXLLmtddeS46x7Rg8eHBp3q9fv2RNahfLadOm1WJKzVZq9/JKO4Y+++yzmWbTvKR24K70tb3ppptK84svvrgmc/pQnz59SvNKu5qvXbu2NH/77beTNc8//3xp/v3vfz9ZM2fOnORY6iSCxYsXJ2sWLFhQmnfo0CFZM2/evOQYRET07NkzOTZ16tSa3ef3v/99cqzS9z0tz5o1a5JjS5YsKc27dOmSrPnDH/5Qmld6Ddscr7/+emn+1ltvJWt222235Ngbb7xRmj/00EPVTYwNeOINAAAAGWm8AQAAICONNwAAAGSk8QYAAICMNN4AAACQkcYbAAAAMnKc2DboueeeK82/9rWvJWv+7u/+rjRvaGhI1px11lml+V577ZWsOfLII5NjbDtSx/y0a9cuWfPHP/6xNJ8yZUpN5tQUtG/fvjS//PLLq77WjBkzkmP/9m//VvX1WqKRI0eW5i+//HKy5rDDDss1nfW88sorpfkDDzyQrPnNb35Tmj/11FO1mNIWOfPMM5NjqeNyKh3TBBtz0UUXJcdSxzRujvHjx9fsWjRvy5YtS44NGTKkNH/44YeTNZ06dSrNf/e73yVrHnzwwdJ84sSJyZo333yzNJ88eXKyptJxYpXq2DKeeAMAAEBGGm8AAADISOMNAAAAGWm8AQAAICONNwAAAGRkV/MmpNJui3fddVdpfttttyVr2rQp/9//uc99LlkzYMCA0vzRRx9N1tA0vPvuu6X5woULt/JM8krtXB4RMXr06NJ81KhRyZoFCxaU5tdee22yZuXKlckxNu473/lOY0+h2Rk4cGDVNVOnTs0wE5qbfv36leZHHXVUTe+T2g36hRdeqOl9aJmefvrp0jx16sPWlPq5vX///smaSicHOLEiH0+8AQAAICONNwAAAGSk8QYAAICMNN4AAACQkcYbAAAAMtJ4AwAAQEaOE9sG9enTpzQ/8cQTkzUHHnhgaZ46MqyS559/Pjn2+OOPV309moZp06Y19hRqKnWETaWjwYYOHVqap46piYg44YQTqpoXNBf3339/Y0+BJuCRRx4pzTt27Fj1tZ566qnk2Kmnnlr19aA56NChQ2le6ciwoiiSY5MnT97iOVHOE28AAADISOMNAAAAGWm8AQAAICONNwAAAGSk8QYAAICM7GqeWe/evUvzc845J1lz/PHHl+Zdu3atyZw+9Oc//7k0X7hwYbKm0g6JbDvq6uqqyiMihgwZUpqff/75tZhSFt/4xjeSY5dccklpvtNOOyVrJk2aVJoPHz68uokBEBERu+yyS2m+OT9P3HjjjcmxlStXVn09aA6mT5/e2FNgE3niDQAAABlpvAEAACAjjTcAAABkpPEGAACAjDTeAAAAkJHGGwAAADJynFgVUsd5DRs2LFmTOjasZ8+etZjSRs2ZMyc5Nm7cuNJ82rRpuabDVlIURVV5RPr7e8KECcma73//+6X50qVLkzWHHHJIaf71r389WdO3b9/SfI899kjWvPLKK6V5pWM3Kh1VAy1V6hjCvffeO1nz1FNP5ZoO26CGhobkWKtWtXvG8+STT9bsWtBcfPGLX2zsKbCJPPEGAACAjDTeAAAAkJHGGwAAADLSeAMAAEBGGm8AAADIqMXuar7rrruW5p/85CeTNf/5n/9Zmu+zzz41mdPGPP3008mx7373u6X5gw8+mKx5//33t3hONB+tW7cuzUeOHJmsOeGEE0rzt956K1mz1157VTexCirtcDtz5szS/NJLL63Z/aElSJ2GUMvdqmka+vXrV5oPGjQoWZP6WWPNmjXJmhtuuKE0X7x4cXpy0EJ9/OMfb+wpsIm8agIAAEBGGm8AAADISOMNAAAAGWm8AQAAICONNwAAAGSk8QYAAICMmsVxYp06dSrNb7755mRN6kiMrbUlf6VjkK699trSfPr06cma1atXb/GcaD5mzZpVms+ePTtZc+CBB1Z9n65du5bmqeP6Klm6dGlybPLkyaX5+eefX/V9gNo49NBDk2MTJ07cehNhq9l5551L89RrQSWvvfZacuzCCy+s+nrQUj3xxBOleaUjHx0p3Dg88QYAAICMNN4AAACQkcYbAAAAMtJ4AwAAQEYabwAAAMhom9vV/OCDDy7NR40alaw56KCDSvNu3brVZE4b8/bbbyfHJkyYUJpfeeWVyZpVq1Zt8Zxo2RYsWFCaH3/88cmas846qzQfPXp0Teb0oeuuu640/6//+q9kzYsvvljTOQCbrq6urrGnAEDCc889V5rPnz8/WVPpFKf6+vrSfMmSJdVNjA144g0AAAAZabwBAAAgI403AAAAZKTxBgAAgIw03gAAAJCRxhsAAAAy2uaOEzvuuOOqyjfX888/X5o//PDDyZq1a9eW5tdee22yZtmyZVXNC3JauHBhcuzyyy+vKgeajx/96EfJsa9+9atbcSZsy+bNm1eaP/nkk8maI444Itd0gAoqHV182223JcfGjRtXmp977rnJmlRfxfo88QYAAICMNN4AAACQkcYbAAAAMtJ4AwAAQEYabwAAAMioriiKYpM+sK4u91xgq9vEb/9S1gTNkTUBG9rcdWFN0Bx5nWgadtxxx+TYPffckxwbNGhQaX7fffcla0477bTSfNWqVcma5mZT1oUn3gAAAJCRxhsAAAAy0ngDAABARhpvAAAAyEjjDQAAABlpvAEAACAjx4nRojkSA9ZnTcCGHCcGf+F1oumrdNTYuHHjSvN/+qd/Stb06dOnNH/++eerm1gT5jgxAAAAaGQabwAAAMhI4w0AAAAZabwBAAAgI403AAAAZGRXc1o0O3PC+qwJ2JBdzeEvvE7AhuxqDgAAAI1M4w0AAAAZabwBAAAgI403AAAAZKTxBgAAgIw03gAAAJDRJh8nBgAAAFTPE28AAADISOMNAAAAGWm8AQAAICONNwAAAGSk8QYAAICMNN4AAACQkcYbAAAAMtJ4AwAAQEYabwAAAMjo/wFHpuqAAXVbogAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Function to display images and their labels\n",
    "def visualize_mnist_images(images, labels, num_images=5):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plotting a grid of images\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(f'Label: {labels[i]}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first 5 images from the training set\n",
    "visualize_mnist_images(X_train, y_train, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Run Times.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun Times.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m df_data\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[1;32mc:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:504\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    503\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    505\u001b[0m         io,\n\u001b[0;32m    506\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    507\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    508\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    509\u001b[0m     )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1563\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1563\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1564\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1565\u001b[0m     )\n\u001b[0;32m   1566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1570\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1419\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1417\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1419\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1420\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1422\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1423\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mdhafeez\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Run Times.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = pd.read_excel('Run Times.xlsx')\n",
    "df_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Name          5 non-null      object        \n",
      " 1   Run Time      5 non-null      float64       \n",
      " 2   Warm Up Time  5 non-null      object        \n",
      " 3   Location      5 non-null      object        \n",
      " 4   Run Date      5 non-null      datetime64[ns]\n",
      " 5   Race Date     5 non-null      datetime64[ns]\n",
      " 6   Rain          5 non-null      bool          \n",
      " 7   Fee           5 non-null      object        \n",
      "dtypes: bool(1), datetime64[ns](2), float64(1), object(4)\n",
      "memory usage: 417.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.duplicated of      Name  Run Time Warm Up Time   Location            Run Date  Race Date  \\\n",
       "0  Alexis    9.2343          3.5   “school” 2023-04-15 12:00:00 2023-06-01   \n",
       "1  Alexis   10.3842          3.5     School 2023-04-22 12:30:00 2023-06-01   \n",
       "2  Alexis    8.1209        3 min  “the gym” 2023-05-10 15:00:00 2023-06-01   \n",
       "3   David    7.2123          2.2   “school” 2023-05-01 15:15:00 2023-06-15   \n",
       "4   David    6.8342            2      “gym” 2023-05-10 16:30:00 2023-06-15   \n",
       "\n",
       "    Rain    Fee  \n",
       "0  False  $0.00  \n",
       "1   True  $0.00  \n",
       "2  False  $2.50  \n",
       "3  False  $0.00  \n",
       "4  False  $2.50  >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name  Run Time Warm Up Time   Location            Run Date  Race Date  \\\n",
      "0  Alexis    9.2343          3.5   “school” 2023-04-15 12:00:00 2023-06-01   \n",
      "1  Alexis   10.3842          3.5     School 2023-04-22 12:30:00 2023-06-01   \n",
      "2  Alexis    8.1209        3 min  “the gym” 2023-05-10 15:00:00 2023-06-01   \n",
      "3   David    7.2123          2.2   “school” 2023-05-01 15:15:00 2023-06-15   \n",
      "4   David    6.8342            2      “gym” 2023-05-10 16:30:00 2023-06-15   \n",
      "\n",
      "    Rain    Fee  \n",
      "0  False  $0.00  \n",
      "1   True  $0.00  \n",
      "2  False  $2.50  \n",
      "3  False  $0.00  \n",
      "4  False  $2.50  \n"
     ]
    }
   ],
   "source": [
    "cleaned = df_data.drop_duplicates(keep='last')\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Name          5 non-null      object        \n",
      " 1   Run Time      5 non-null      float64       \n",
      " 2   Warm Up Time  5 non-null      object        \n",
      " 3   Location      5 non-null      object        \n",
      " 4   Run Date      5 non-null      datetime64[ns]\n",
      " 5   Race Date     5 non-null      datetime64[ns]\n",
      " 6   Rain          5 non-null      bool          \n",
      " 7   Fee           5 non-null      object        \n",
      "dtypes: bool(1), datetime64[ns](2), float64(1), object(4)\n",
      "memory usage: 417.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "cleaned.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to detailed_course_links_with_images.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = 'https://www.dtims.intan.my/search_course?page='\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 10  # Adjust this number based on the total pages you want to scrape\n",
    "\n",
    "# List to store course data\n",
    "course_data = []\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(1, num_pages + 1):\n",
    "    # Construct the full URL for the current page\n",
    "    url = base_url + str(page)\n",
    "    \n",
    "    # Send a request to the website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all course items (assuming each course item has a common parent class or identifier)\n",
    "    course_items = soup.find_all('div', class_='mc-item mc-item-2')  # Adjust this selector to match the actual structure\n",
    "\n",
    "    # Extract data for each course item\n",
    "    for item in course_items:\n",
    "        # Extracting course URL\n",
    "        link_tag = item.find('a', href=True)\n",
    "        if link_tag and 'detail_course' in link_tag['href']:\n",
    "            course_url = link_tag['href']\n",
    "            course_code = course_url.split('/')[-1]  # Extract course code from URL\n",
    "\n",
    "            # Sending a request to the course detail page\n",
    "            course_response = requests.get(course_url)\n",
    "            course_soup = BeautifulSoup(course_response.text, 'html.parser')\n",
    "\n",
    "            # Extracting \"Kod Kursus\" and \"Nama Kursus\" from the detail page\n",
    "            sub_banner = course_soup.find('section', class_='sub-banner-course')\n",
    "            if sub_banner:\n",
    "                nama_kursus = sub_banner.find('h2').text.strip()\n",
    "                kod_kursus = nama_kursus.split(']')[0].strip('[').strip()  # Extract \"Kod Kursus\" from text\n",
    "\n",
    "            # Extracting course image\n",
    "            image_tag = course_soup.find('div', class_='video-course-intro').find('img')\n",
    "            course_image_url = image_tag['src'] if image_tag else 'N/A'  # Extract image URL if available\n",
    "\n",
    "            # Append the extracted data to the list\n",
    "            course_data.append({\n",
    "                'course_code': course_code,\n",
    "                'course_url': course_url,\n",
    "                'kod_kursus': kod_kursus if sub_banner else 'N/A',\n",
    "                'nama_kursus': nama_kursus if sub_banner else 'N/A',\n",
    "                'course_image_url': course_image_url  # Add image URL\n",
    "            })\n",
    "\n",
    "# Saving the data into a CSV file\n",
    "csv_filename = 'detailed_course_links_with_images.csv'\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['course_code', 'course_url', 'kod_kursus', 'nama_kursus', 'course_image_url'])\n",
    "    writer.writeheader()\n",
    "    for course in course_data:\n",
    "        writer.writerow(course)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_code</th>\n",
       "      <th>course_url</th>\n",
       "      <th>kod_kursus</th>\n",
       "      <th>nama_kursus</th>\n",
       "      <th>course_image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xhqa6jQ44CSyTMWT</td>\n",
       "      <td>https://www.dtims.intan.my/detail_course/Xhqa6...</td>\n",
       "      <td>IEA0004</td>\n",
       "      <td>[ IEA0004 ] KURSUS PENGENDALIAN KREN KAMERA PR...</td>\n",
       "      <td>https://admin.dtims.intan.my/upload/banner/bn-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>odSCLQjFM5JzGu5w</td>\n",
       "      <td>https://www.dtims.intan.my/detail_course/odSCL...</td>\n",
       "      <td>BZA0074</td>\n",
       "      <td>[ BZA0074 ] Kursus Suntingan Video [ Siri 1/20...</td>\n",
       "      <td>https://admin.dtims.intan.my/upload/banner/bn-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EYrQa6ano2nHaAh1</td>\n",
       "      <td>https://www.dtims.intan.my/detail_course/EYrQa...</td>\n",
       "      <td>UZA0071</td>\n",
       "      <td>[ UZA0071 ] Creative Content Creator [ Siri 1/...</td>\n",
       "      <td>https://admin.dtims.intan.my/upload/banner/bn-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QlmPrwkeN3XccSfu</td>\n",
       "      <td>https://www.dtims.intan.my/detail_course/QlmPr...</td>\n",
       "      <td>BZA0073</td>\n",
       "      <td>[ BZA0073 ] Kursus Rekabentuk Grafik dan Penyu...</td>\n",
       "      <td>https://admin.dtims.intan.my/upload/banner/bn-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NK0r2N0b7V7wkepI</td>\n",
       "      <td>https://www.dtims.intan.my/detail_course/NK0r2...</td>\n",
       "      <td>FZA0010</td>\n",
       "      <td>[ FZA0010 ] Kursus Asas Keusahawanan [ Siri 1/...</td>\n",
       "      <td>https://admin.dtims.intan.my/upload/banner/bn-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_code                                         course_url  \\\n",
       "0  Xhqa6jQ44CSyTMWT  https://www.dtims.intan.my/detail_course/Xhqa6...   \n",
       "1  odSCLQjFM5JzGu5w  https://www.dtims.intan.my/detail_course/odSCL...   \n",
       "2  EYrQa6ano2nHaAh1  https://www.dtims.intan.my/detail_course/EYrQa...   \n",
       "3  QlmPrwkeN3XccSfu  https://www.dtims.intan.my/detail_course/QlmPr...   \n",
       "4  NK0r2N0b7V7wkepI  https://www.dtims.intan.my/detail_course/NK0r2...   \n",
       "\n",
       "  kod_kursus                                        nama_kursus  \\\n",
       "0    IEA0004  [ IEA0004 ] KURSUS PENGENDALIAN KREN KAMERA PR...   \n",
       "1    BZA0074  [ BZA0074 ] Kursus Suntingan Video [ Siri 1/20...   \n",
       "2    UZA0071  [ UZA0071 ] Creative Content Creator [ Siri 1/...   \n",
       "3    BZA0073  [ BZA0073 ] Kursus Rekabentuk Grafik dan Penyu...   \n",
       "4    FZA0010  [ FZA0010 ] Kursus Asas Keusahawanan [ Siri 1/...   \n",
       "\n",
       "                                    course_image_url  \n",
       "0  https://admin.dtims.intan.my/upload/banner/bn-...  \n",
       "1  https://admin.dtims.intan.my/upload/banner/bn-...  \n",
       "2  https://admin.dtims.intan.my/upload/banner/bn-...  \n",
       "3  https://admin.dtims.intan.my/upload/banner/bn-...  \n",
       "4  https://admin.dtims.intan.my/upload/banner/bn-...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_image = pd.read_csv('detailed_course_links_with_images.csv')\n",
    "df_noimage = pd.read_csv('Updated_Courses_with_URLs.csv')\n",
    "\n",
    "df_image.head()\n",
    "# df_noimage.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved as 'Updated_Courses_with_Image_URLs.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into dataframes\n",
    "df_image = pd.read_csv('detailed_course_links_with_images.csv')\n",
    "df_noimage = pd.read_csv('Updated_Courses_with_URLs.csv')\n",
    "\n",
    "# Merge the two dataframes on the 'course_url' column\n",
    "df_updated = df_noimage.merge(df_image[['course_url', 'course_image_url']], on='course_url', how='left')\n",
    "\n",
    "# Update the 'course_image_url' in df_noimage with the values from df_image\n",
    "df_noimage['course_image_url'] = df_updated['course_image_url']\n",
    "\n",
    "# Save the updated dataframe back to CSV\n",
    "df_noimage.to_csv('Updated_Courses_with_Image_URLs.csv', index=False)\n",
    "\n",
    "print(\"Updated CSV file saved as 'Updated_Courses_with_Image_URLs.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to detailed_course_info.csv\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import csv\n",
    "\n",
    "# # Base URL of the page to scrape\n",
    "# base_url = 'https://www.dtims.intan.my/search_course?page='\n",
    "\n",
    "# # Number of pages to scrape\n",
    "# num_pages = 10  # Adjust this number based on the total pages you want to scrape\n",
    "\n",
    "# # List to store course data\n",
    "# course_data = []\n",
    "\n",
    "# # Loop through each page\n",
    "# for page in range(1, num_pages + 1):\n",
    "#     # Construct the full URL for the current page\n",
    "#     url = base_url + str(page)\n",
    "    \n",
    "#     # Send a request to the website\n",
    "#     response = requests.get(url)\n",
    "    \n",
    "#     # Parse the HTML content\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "#     # Find all course items (assuming each course item has a common parent class or identifier)\n",
    "#     course_items = soup.find_all('div', class_='mc-item mc-item-2')  # Adjust this selector to match the actual structure\n",
    "\n",
    "#     # Extract data for each course item\n",
    "#     for item in course_items:\n",
    "#         # Extracting course URL\n",
    "#         link_tag = item.find('a', href=True)\n",
    "#         if link_tag and 'detail_course' in link_tag['href']:\n",
    "#             course_url = link_tag['href']\n",
    "#             course_code = course_url.split('/')[-1]  # Extract course code from URL\n",
    "\n",
    "#             # Sending a request to the course detail page\n",
    "#             course_response = requests.get(course_url)\n",
    "#             course_soup = BeautifulSoup(course_response.text, 'html.parser')\n",
    "\n",
    "#             # Extracting \"Kod Kursus\" and \"Nama Kursus\" from the detail page\n",
    "#             sub_banner = course_soup.find('section', class_='sub-banner-course')\n",
    "#             if sub_banner:\n",
    "#                 nama_kursus = sub_banner.find('h2').text.strip()\n",
    "#                 kod_kursus = nama_kursus.split(']')[0].strip('[').strip()  # Extract \"Kod Kursus\" from text\n",
    "\n",
    "#             # Extracting \"Sinopsis Kursus\"\n",
    "#             sinopsis_section = course_soup.find('div', class_='card-header', string=' Sinopsis Kursus')\n",
    "#             sinopsis_kursus = ''\n",
    "#             if sinopsis_section:\n",
    "#                 sinopsis_kursus = sinopsis_section.find_next('div', class_='card-body').get_text(separator=' ', strip=True)\n",
    "\n",
    "#             # Extracting \"Objektif Kursus\"\n",
    "#             objektif_section = course_soup.find('div', class_='card-header', string=' Objektif Kursus')\n",
    "#             objektif_kursus = ''\n",
    "#             if objektif_section:\n",
    "#                 objektif_kursus = objektif_section.find_next('div', class_='card-body').get_text(separator=' ', strip=True)\n",
    "\n",
    "#             # Append the extracted data to the list\n",
    "#             course_data.append({\n",
    "#                 'course_code': course_code,\n",
    "#                 'course_url': course_url,\n",
    "#                 'kod_kursus': kod_kursus if sub_banner else 'N/A',\n",
    "#                 'nama_kursus': nama_kursus if sub_banner else 'N/A',\n",
    "#                 'sinopsis_kursus': sinopsis_kursus,\n",
    "#                 'objektif_kursus': objektif_kursus\n",
    "#             })\n",
    "\n",
    "# # Saving the data into a CSV file\n",
    "# csv_filename = 'detailed_course_info.csv'\n",
    "# with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.DictWriter(file, fieldnames=['course_code', 'course_url', 'kod_kursus', 'nama_kursus', 'sinopsis_kursus', 'objektif_kursus'])\n",
    "#     writer.writeheader()\n",
    "#     for course in course_data:\n",
    "#         writer.writerow(course)\n",
    "\n",
    "# print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title  \\\n",
      "0      UI/UX Designer Course 1   \n",
      "1              Tester Course 2   \n",
      "2  Frontend Developer Course 3   \n",
      "3              DevOps Course 4   \n",
      "4      UI/UX Designer Course 5   \n",
      "\n",
      "                               improved_course_title  \n",
      "0            Explore prototyping for designer: adobe  \n",
      "1                   Explore bug for tester: bugzilla  \n",
      "2                    Explore css for developer: sass  \n",
      "3          A Guide to automation for devops: jenkins  \n",
      "4  Advanced Techniques in design for designer: adobe  \n"
     ]
    }
   ],
   "source": [
    "### modify the course title to make it more organic and save it into updated_courses.csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv(\"courses.csv\")\n",
    "\n",
    "# List of action words to choose from\n",
    "action_words = [\"Mastering\", \"Learn\", \"A Guide to\", \"Understanding\", \"Explore\", \"Advanced Techniques in\"]\n",
    "\n",
    "# Function to extract top N keywords using TF-IDF\n",
    "def extract_keywords_advanced(text, top_n=3):\n",
    "    if isinstance(text, str):\n",
    "        # Using TF-IDF to extract top keywords\n",
    "        tfidf = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "        tfidf_matrix = tfidf.fit_transform([text])  # Should pass a list of strings for TF-IDF\n",
    "        terms = tfidf.get_feature_names_out()\n",
    "        return sorted(terms[:top_n])\n",
    "    return []\n",
    "\n",
    "# Function to generate the new course title based on analysis\n",
    "def generate_advanced_course_title(row):\n",
    "    # Handle missing data with empty strings\n",
    "    description = row.get('description', '')\n",
    "    skills = row.get('skills_required', '')\n",
    "    sub_skills = row.get('sub_skills_required', '')\n",
    "    job_role = row.get('job_role', '')\n",
    "\n",
    "    # Extract keywords from each relevant field\n",
    "    description_keywords = extract_keywords_advanced(description)\n",
    "    skill_keywords = extract_keywords_advanced(skills)\n",
    "    sub_skill_keywords = extract_keywords_advanced(sub_skills)\n",
    "    job_role_keywords = extract_keywords_advanced(job_role)\n",
    "\n",
    "    # Select primary keywords for the title\n",
    "    primary_skill = skill_keywords[0] if skill_keywords else \"\"\n",
    "    specific_focus = sub_skill_keywords[0] if sub_skill_keywords else \"\"\n",
    "    job_role_keyword = job_role_keywords[0] if job_role_keywords else \"\"\n",
    "\n",
    "    # Randomly choose an action word\n",
    "    action_word = random.choice(action_words)\n",
    "\n",
    "    # Create a more structured and informative title\n",
    "    new_title = f\"{action_word} {primary_skill} for {job_role_keyword}: {specific_focus}\"\n",
    "    \n",
    "    # Clean up the title to make it more readable\n",
    "    new_title = re.sub(r'\\s+', ' ', new_title).strip()\n",
    "    \n",
    "    return new_title\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['improved_course_title'] = df.apply(generate_advanced_course_title, axis=1)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(df[['title', 'improved_course_title']].head())\n",
    "\n",
    "# Optionally, save the result to a new CSV file\n",
    "df.to_csv(\"updated_courses.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title  \\\n",
      "0      UI/UX Designer Course 1   \n",
      "1              Tester Course 2   \n",
      "2  Frontend Developer Course 3   \n",
      "3              DevOps Course 4   \n",
      "4      UI/UX Designer Course 5   \n",
      "\n",
      "                                concise_course_title  \n",
      "0  Master UI/UX Designer: Prototyping, Wireframin...  \n",
      "1  Understand Tester: Bug, Techniques, Testing, T...  \n",
      "2  Understand Frontend Developer: Css, Javascript...  \n",
      "3  Understand DevOps: Automation, Continuous, Dep...  \n",
      "4  Master UI/UX Designer: Design, Thinking, Adobe...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv(\"courses.csv\")\n",
    "\n",
    "# List of concise action words\n",
    "action_words = [\"Master\", \"Learn\", \"Explore\", \"Understand\"]\n",
    "\n",
    "# Dictionary of skill synonyms\n",
    "synonym_dict = {\n",
    "    \"js\": \"JavaScript\",\n",
    "    \"html5\": \"HTML\",\n",
    "    \"css3\": \"CSS\",\n",
    "    \"xd\": \"Adobe XD\"\n",
    "}\n",
    "\n",
    "# Function to normalize and merge similar skills\n",
    "def normalize_skills(skills_list):\n",
    "    # Normalize by making everything lowercase and removing leading/trailing spaces\n",
    "    skills_list = [skill.lower().strip() for skill in skills_list]\n",
    "    \n",
    "    # Replace synonyms using the synonym dictionary\n",
    "    normalized_skills = []\n",
    "    for skill in skills_list:\n",
    "        skill = synonym_dict.get(skill, skill)  # Replace synonym if found\n",
    "        if skill not in normalized_skills:  # Avoid duplicates\n",
    "            normalized_skills.append(skill)\n",
    "    \n",
    "    # Capitalize first letter of each skill for better readability\n",
    "    return [skill.title() for skill in normalized_skills]\n",
    "\n",
    "# Function to extract top N keywords using TF-IDF\n",
    "def extract_keywords_advanced(text, top_n=5):\n",
    "    if isinstance(text, str):\n",
    "        # Using TF-IDF to extract top keywords\n",
    "        tfidf = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "        tfidf_matrix = tfidf.fit_transform([text])\n",
    "        terms = tfidf.get_feature_names_out()\n",
    "        return sorted(terms[:top_n])\n",
    "    return []\n",
    "\n",
    "# Function to generate the course title based on all skills and sub-skills, merged\n",
    "def generate_concise_course_title(row):\n",
    "    # Handle missing data with empty strings\n",
    "    description = row.get('description', '')\n",
    "    skills = row.get('skills_required', '')\n",
    "    sub_skills = row.get('sub_skills_required', '')\n",
    "    job_role = row.get('job_role', '')\n",
    "\n",
    "    # Extract keywords from each relevant field\n",
    "    skill_keywords = extract_keywords_advanced(skills)\n",
    "    sub_skill_keywords = extract_keywords_advanced(sub_skills)\n",
    "\n",
    "    # Merge and normalize all skills and sub-skills\n",
    "    all_skills = skill_keywords + sub_skill_keywords\n",
    "    merged_skills = normalize_skills(all_skills)\n",
    "    \n",
    "    # Randomly choose an action word\n",
    "    action_word = random.choice(action_words)\n",
    "\n",
    "    # Create a concise title using merged skills\n",
    "    if merged_skills:\n",
    "        skills_str = ', '.join(merged_skills)\n",
    "        new_title = f\"{action_word} {job_role}: {skills_str}\"\n",
    "    else:\n",
    "        new_title = f\"{action_word} {job_role}\"\n",
    "    \n",
    "    # Clean up the title to make it more readable\n",
    "    new_title = re.sub(r'\\s+', ' ', new_title).strip()\n",
    "    \n",
    "    return new_title\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['concise_course_title'] = df.apply(generate_concise_course_title, axis=1)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(df[['title', 'concise_course_title']].head())\n",
    "\n",
    "# Optionally, save the result to a new CSV file\n",
    "df.to_csv(\"concise_merged_courses.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  course_id                                            title  job_role  \\\n",
      "0   KCA0038          Fundamentals of Enterprise Architecture       NaN   \n",
      "1   KCA0044  DEVELOPMENT OF STRATEGIC PLAN DEVELOPMENT (PSP)       NaN   \n",
      "2   KCA0054                                  Cloud Computing       NaN   \n",
      "3   KCA0051                 IT Change Management Foundations       NaN   \n",
      "4   KCA0083               Refresher DGCCR Certified Trainers       NaN   \n",
      "\n",
      "   skills_required  sub_skills_required  \\\n",
      "0              NaN                  NaN   \n",
      "1              NaN                  NaN   \n",
      "2              NaN                  NaN   \n",
      "3              NaN                  NaN   \n",
      "4              NaN                  NaN   \n",
      "\n",
      "                                         description  subdomain_id  \\\n",
      "0  Dive into the world of enterprise architecture...             1   \n",
      "1  Master the art of strategic planning with a fo...             1   \n",
      "2  Explore the essentials of cloud computing, inc...             1   \n",
      "3  This course offers a deep dive into It Change,...             1   \n",
      "4  Enhance your skills as a DGCCR-certified train...             1   \n",
      "\n",
      "   subdomain_name_x          subdomain_name_y  \n",
      "0               NaN  ICT Strategic Management  \n",
      "1               NaN  ICT Strategic Management  \n",
      "2               NaN  ICT Strategic Management  \n",
      "3               NaN  ICT Strategic Management  \n",
      "4               NaN  ICT Strategic Management  \n"
     ]
    }
   ],
   "source": [
    "# file : merge_subdomain.py\n",
    "# \n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the courses and subdomain data\n",
    "# courses_df = pd.read_csv('courses_1.csv', encoding='latin1')\n",
    "# subdomain_df = pd.read_csv('subdomain_df.csv', encoding='latin1')\n",
    "\n",
    "# # Merge the data to insert subdomain names\n",
    "# merged_courses_df = courses_df.merge(subdomain_df[['subdomain_id', 'subdomain_name']], on='subdomain_id', how='left')\n",
    "\n",
    "# # Save the updated DataFrame with subdomain names inserted\n",
    "# merged_courses_df.to_csv('Updated_Courses_with_Subdomain.csv', index=False)\n",
    "\n",
    "# # Print the first few rows to verify the merge\n",
    "# print(merged_courses_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  This course covers essential skills for the ro...   \n",
      "1  This course covers essential skills for the ro...   \n",
      "2  This course covers essential skills for the ro...   \n",
      "3  This course covers essential skills for the ro...   \n",
      "4  This course covers essential skills for the ro...   \n",
      "\n",
      "                                modified_description  \n",
      "0  A successful UI/UX Designer relies heavily on ...  \n",
      "1  This course offers comprehensive training for ...  \n",
      "2  This course offers comprehensive training for ...  \n",
      "3  Learn the essentials of DevOps, focusing on Au...  \n",
      "4  A successful UI/UX Designer relies heavily on ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv(\"courses.csv\")\n",
    "\n",
    "# Expanded list of description templates\n",
    "description_templates = [\n",
    "    \"As a {job_role}, mastering {skills} is crucial. This course provides hands-on training with tools like {sub_skills} to create user-centric designs.\",\n",
    "    \"The {job_role} role requires a strong foundation in {skills}. In this course, you will learn to leverage tools like {sub_skills} for efficient design.\",\n",
    "    \"Learn the essentials of {job_role}, focusing on {skills}. You'll dive into industry-standard tools like {sub_skills} to build professional-level projects.\",\n",
    "    \"Gain the skills needed to excel as a {job_role}, including {skills}. This course includes practical exercises with tools like {sub_skills}.\",\n",
    "    \"Explore the key skills of {job_role}, such as {skills}. You’ll also work with powerful tools like {sub_skills} to enhance your design abilities.\",\n",
    "    \"In the role of a {job_role}, you will need expertise in {skills}. This course will guide you through practical applications of tools like {sub_skills}, helping you build real-world projects.\",\n",
    "    \"A successful {job_role} relies heavily on {skills}. With this course, you'll not only learn the theory but also gain hands-on experience with tools such as {sub_skills} to bring your ideas to life.\",\n",
    "    \"This course offers comprehensive training for aspiring {job_role}s, focusing on key skills such as {skills}. You’ll also get the chance to work with industry-standard tools like {sub_skills}.\",\n",
    "    \"Designed for {job_role}s, this course covers core skills like {skills}, and teaches you how to apply them using essential tools like {sub_skills}. It's an ideal step toward mastering your craft.\",\n",
    "    \"Step into the role of a {job_role} with confidence, learning critical skills such as {skills}. This course will provide you with hands-on experience using {sub_skills}, the tools you need to succeed.\",\n",
    "    # Add more templates as needed from the list above...\n",
    "]\n",
    "\n",
    "# Function to modify the course description\n",
    "def modify_description(row):\n",
    "    # Extract information from the row\n",
    "    job_role = row['job_role']\n",
    "    skills = row['skills_required']\n",
    "    sub_skills = row['sub_skills_required']\n",
    "    \n",
    "    # Randomly choose a description template\n",
    "    template = random.choice(description_templates)\n",
    "    \n",
    "    # Fill the template with values\n",
    "    new_description = template.format(\n",
    "        job_role=job_role,\n",
    "        skills=skills,\n",
    "        sub_skills=sub_skills\n",
    "    )\n",
    "    \n",
    "    return new_description\n",
    "\n",
    "# Apply the function to the dataframe to modify descriptions\n",
    "df['modified_description'] = df.apply(modify_description, axis=1)\n",
    "\n",
    "# Display the updated dataset with new descriptions\n",
    "print(df[['description', 'modified_description']].head())\n",
    "\n",
    "# Optionally, save the result to a new CSV file\n",
    "df.to_csv(\"modified_course_descriptions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Expanded list of job roles and their required skills and sub-skills\n",
    "job_roles = [\n",
    "    'Data Scientist', 'Data Analyst', 'Data Engineer', 'Web Developer',\n",
    "    'Backend Developer', 'Frontend Developer', 'Business Analyst',\n",
    "    'Scrum Master', 'Software Engineer', 'UI/UX Designer', 'Admin',\n",
    "    'DevOps Engineer', 'Project Manager', 'Tester', 'Cloud Architect',\n",
    "    'AI Engineer', 'Blockchain Developer', 'Cybersecurity Analyst',\n",
    "    'Mobile Developer', 'Machine Learning Engineer', 'Database Administrator',\n",
    "    'Network Engineer', 'Security Engineer', 'Site Reliability Engineer',\n",
    "    'IT Support Specialist', 'Cloud Security Engineer', 'IoT Developer',\n",
    "    'Data Architect', 'Product Manager', 'IT Manager', 'Solutions Architect',\n",
    "    'Penetration Tester', 'Software Architect', 'Ethical Hacker', \n",
    "    'Systems Analyst', 'Quality Assurance Engineer', 'Technical Writer'\n",
    "]\n",
    "\n",
    "# Skills dictionary\n",
    "skills_dict = {\n",
    "    'Data Scientist': {\n",
    "        'main': ['Python', 'Statistics', 'Machine Learning'],\n",
    "        'sub': ['Pandas', 'Scikit-learn', 'TensorFlow', 'Keras']\n",
    "    },\n",
    "    'Data Analyst': {\n",
    "        'main': ['Excel', 'SQL', 'Data Visualization'],\n",
    "        'sub': ['Tableau', 'Power BI', 'Matplotlib', 'Seaborn']\n",
    "    },\n",
    "    'Data Engineer': {\n",
    "        'main': ['ETL', 'Big Data', 'SQL'],\n",
    "        'sub': ['Apache Spark', 'Hadoop', 'Kafka', 'Airflow']\n",
    "    },\n",
    "    'Web Developer': {\n",
    "        'main': ['HTML', 'CSS', 'JavaScript'],\n",
    "        'sub': ['React', 'Angular', 'Vue.js', 'Bootstrap']\n",
    "    },\n",
    "    'Backend Developer': {\n",
    "        'main': ['Node.js', 'API Development', 'Database Management'],\n",
    "        'sub': ['Express.js', 'MongoDB', 'SQL Server', 'Django']\n",
    "    },\n",
    "    'Frontend Developer': {\n",
    "        'main': ['HTML', 'CSS', 'JavaScript'],\n",
    "        'sub': ['Sass', 'Webpack', 'JQuery', 'Vue.js']\n",
    "    },\n",
    "    'Business Analyst': {\n",
    "        'main': ['Requirement Gathering', 'Data Analysis', 'Reporting'],\n",
    "        'sub': ['JIRA', 'Confluence', 'SQL', 'Excel']\n",
    "    },\n",
    "    'Scrum Master': {\n",
    "        'main': ['Agile Methodology', 'Scrum Practices'],\n",
    "        'sub': ['JIRA', 'Confluence', 'Retrospectives', 'Stand-ups']\n",
    "    },\n",
    "    'Software Engineer': {\n",
    "        'main': ['Programming', 'Software Design', 'Testing'],\n",
    "        'sub': ['Git', 'JUnit', 'Docker', 'Kubernetes']\n",
    "    },\n",
    "    'UI/UX Designer': {\n",
    "        'main': ['Design Thinking', 'Wireframing', 'Prototyping'],\n",
    "        'sub': ['Sketch', 'Figma', 'Adobe XD', 'InVision']\n",
    "    },\n",
    "    'Admin': {\n",
    "        'main': ['System Administration', 'Network Management'],\n",
    "        'sub': ['Linux', 'Windows Server', 'Networking', 'Scripting']\n",
    "    },\n",
    "    'DevOps Engineer': {\n",
    "        'main': ['Continuous Integration', 'Continuous Deployment', 'Automation'],\n",
    "        'sub': ['Jenkins', 'Docker', 'Kubernetes', 'Ansible']\n",
    "    },\n",
    "    'Project Manager': {\n",
    "        'main': ['Project Planning', 'Risk Management', 'Budgeting'],\n",
    "        'sub': ['MS Project', 'Asana', 'Trello', 'Gantt Charts']\n",
    "    },\n",
    "    'Tester': {\n",
    "        'main': ['Testing Techniques', 'Automation', 'Bug Tracking'],\n",
    "        'sub': ['Selenium', 'JUnit', 'Bugzilla', 'TestRail']\n",
    "    },\n",
    "    'Cloud Architect': {\n",
    "        'main': ['Cloud Computing', 'Infrastructure Design', 'Security'],\n",
    "        'sub': ['AWS', 'Azure', 'Google Cloud', 'Terraform']\n",
    "    },\n",
    "    'AI Engineer': {\n",
    "        'main': ['Artificial Intelligence', 'Deep Learning', 'Natural Language Processing'],\n",
    "        'sub': ['TensorFlow', 'Keras', 'PyTorch', 'OpenCV']\n",
    "    },\n",
    "    'Blockchain Developer': {\n",
    "        'main': ['Smart Contracts', 'Distributed Ledger Technology', 'Security'],\n",
    "        'sub': ['Solidity', 'Ethereum', 'Hyperledger', 'Truffle']\n",
    "    },\n",
    "    'Cybersecurity Analyst': {\n",
    "        'main': ['Security Assessment', 'Penetration Testing', 'Incident Response'],\n",
    "        'sub': ['Nmap', 'Wireshark', 'Metasploit', 'Splunk']\n",
    "    },\n",
    "    'Mobile Developer': {\n",
    "        'main': ['Mobile App Development', 'UI/UX Design', 'API Integration'],\n",
    "        'sub': ['React Native', 'Flutter', 'Swift', 'Kotlin']\n",
    "    },\n",
    "    'Machine Learning Engineer': {\n",
    "        'main': ['Machine Learning', 'Data Preprocessing', 'Model Optimization'],\n",
    "        'sub': ['TensorFlow', 'PyTorch', 'Keras', 'Scikit-learn']\n",
    "    },\n",
    "    'Database Administrator': {\n",
    "        'main': ['Database Management', 'SQL', 'Backup and Recovery'],\n",
    "        'sub': ['MySQL', 'Oracle', 'SQL Server', 'PostgreSQL']\n",
    "    },\n",
    "    'Network Engineer': {\n",
    "        'main': ['Networking', 'Routing and Switching', 'Security'],\n",
    "        'sub': ['Cisco', 'Juniper', 'BGP', 'Firewall Management']\n",
    "    },\n",
    "    'Security Engineer': {\n",
    "        'main': ['Network Security', 'Data Encryption', 'Incident Response'],\n",
    "        'sub': ['SSL', 'TLS', 'PKI', 'Firewalls']\n",
    "    },\n",
    "    'Site Reliability Engineer': {\n",
    "        'main': ['System Reliability', 'Infrastructure Automation', 'Monitoring'],\n",
    "        'sub': ['Prometheus', 'Grafana', 'Kubernetes', 'Ansible']\n",
    "    },\n",
    "    'IT Support Specialist': {\n",
    "        'main': ['Troubleshooting', 'Technical Support', 'Customer Service'],\n",
    "        'sub': ['Helpdesk Tools', 'Windows', 'MacOS', 'Linux']\n",
    "    },\n",
    "    'Cloud Security Engineer': {\n",
    "        'main': ['Cloud Security', 'Compliance', 'Risk Management'],\n",
    "        'sub': ['IAM', 'AWS Security', 'Azure Security', 'SOC 2']\n",
    "    },\n",
    "    'IoT Developer': {\n",
    "        'main': ['IoT Protocols', 'Embedded Systems', 'Device Integration'],\n",
    "        'sub': ['MQTT', 'Zigbee', 'LoRaWAN', 'Arduino']\n",
    "    },\n",
    "    'Data Architect': {\n",
    "        'main': ['Data Modeling', 'Database Design', 'ETL'],\n",
    "        'sub': ['SQL', 'Hadoop', 'Data Lakes', 'Amazon Redshift']\n",
    "    },\n",
    "    'Product Manager': {\n",
    "        'main': ['Product Roadmap', 'Agile Methodologies', 'Market Analysis'],\n",
    "        'sub': ['JIRA', 'Confluence', 'Trello', 'User Stories']\n",
    "    },\n",
    "    'IT Manager': {\n",
    "        'main': ['IT Strategy', 'Team Leadership', 'Budgeting'],\n",
    "        'sub': ['ITIL', 'Change Management', 'Disaster Recovery', 'SLAs']\n",
    "    },\n",
    "    'Solutions Architect': {\n",
    "        'main': ['Solution Design', 'Technical Leadership', 'Integration'],\n",
    "        'sub': ['AWS', 'Azure', 'GCP', 'API Design']\n",
    "    },\n",
    "    'Penetration Tester': {\n",
    "        'main': ['Penetration Testing', 'Vulnerability Assessment', 'Security Analysis'],\n",
    "        'sub': ['Metasploit', 'Burp Suite', 'Kali Linux', 'OWASP ZAP']\n",
    "    },\n",
    "    'Software Architect': {\n",
    "        'main': ['Software Design', 'System Architecture', 'Scalability'],\n",
    "        'sub': ['UML', 'Microservices', 'Cloud Architecture', 'Docker']\n",
    "    },\n",
    "    'Ethical Hacker': {\n",
    "        'main': ['Hacking Techniques', 'Penetration Testing', 'Network Security'],\n",
    "        'sub': ['Kali Linux', 'Nmap', 'Wireshark', 'Burp Suite']\n",
    "    },\n",
    "    'Systems Analyst': {\n",
    "        'main': ['System Analysis', 'Requirements Gathering', 'Solution Design'],\n",
    "        'sub': ['UML', 'BPMN', 'JIRA', 'Confluence']\n",
    "    },\n",
    "    'Quality Assurance Engineer': {\n",
    "        'main': ['Testing', 'Automation', 'Quality Assurance'],\n",
    "        'sub': ['Selenium', 'Cypress', 'JIRA', 'Postman']\n",
    "    },\n",
    "    'Technical Writer': {\n",
    "        'main': ['Documentation', 'Technical Writing', 'User Manuals'],\n",
    "        'sub': ['Markdown', 'Confluence', 'API Documentation', 'GitHub']\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of difficulty levels\n",
    "difficulty_levels = ['Beginner', 'Intermediate', 'Advanced']\n",
    "\n",
    "# List of title variations\n",
    "title_variations = [\n",
    "    'Mastering {job_role}', 'Introduction to {job_role}', '{job_role} Essentials',\n",
    "    '{job_role}: Advanced Techniques', 'Complete {job_role} Course', 'Foundations of {job_role}'\n",
    "]\n",
    "\n",
    "# List of description templates for variety\n",
    "description_templates = [\n",
    "    \"As a {job_role}, mastering {skills_required} is essential. You'll also gain hands-on experience with tools like {sub_skills_required}.\",\n",
    "    \"In this {job_role} course, you'll learn {skills_required} and work with tools like {sub_skills_required}.\",\n",
    "    \"This course equips you with the key skills of a {job_role}, focusing on {skills_required} and tools like {sub_skills_required}.\",\n",
    "    \"Gain expertise as a {job_role} by mastering {skills_required} and working with {sub_skills_required}.\",\n",
    "    \"This {job_role} course helps you develop proficiency in {skills_required}, along with experience using {sub_skills_required}.\",\n",
    "    \"Develop critical skills such as {skills_required} and gain practical knowledge with {sub_skills_required} in this {job_role} course.\"\n",
    "]\n",
    "\n",
    "# Function to generate course duration based on difficulty level\n",
    "def generate_course_duration(difficulty):\n",
    "    if difficulty == 'Beginner':\n",
    "        return np.random.randint(5, 21)  # Duration between 5 and 20 hours\n",
    "    elif difficulty == 'Intermediate':\n",
    "        return np.random.randint(20, 41)  # Duration between 20 and 40 hours\n",
    "    else:  # Advanced\n",
    "        return np.random.randint(40, 61)  # Duration between 40 and 60 hours\n",
    "\n",
    "# Function to generate courses for a specific job role\n",
    "def generate_courses_data(num_courses):\n",
    "    courses = []\n",
    "    course_id = 1\n",
    "    for _ in range(num_courses):\n",
    "        job_role = np.random.choice(job_roles)  # Randomly select a job role for multiple courses\n",
    "        main_skills = skills_dict[job_role]['main']\n",
    "        sub_skills = skills_dict[job_role]['sub']\n",
    "        difficulty = np.random.choice(difficulty_levels)  # Randomly select difficulty level\n",
    "        duration = generate_course_duration(difficulty)  # Generate course duration based on difficulty level\n",
    "        \n",
    "        # Generate multiple courses for the same job role with varying skills and sub-skills\n",
    "        for i in range(np.random.randint(1, 4)):  # Generate between 1 to 3 courses per job role\n",
    "            skills_required = ', '.join(np.random.choice(main_skills, size=np.random.randint(1, 3), replace=False))\n",
    "            sub_skills_required = ', '.join(np.random.choice(sub_skills, size=np.random.randint(1, 3), replace=False))\n",
    "            \n",
    "            # Randomly select a course title template and format it\n",
    "            course_title_template = random.choice(title_variations)\n",
    "            course_title = course_title_template.format(job_role=job_role)\n",
    "            \n",
    "            # Randomly select a course description template and format it\n",
    "            course_description_template = random.choice(description_templates)\n",
    "            course_description = course_description_template.format(\n",
    "                job_role=job_role,\n",
    "                skills_required=skills_required,\n",
    "                sub_skills_required=sub_skills_required\n",
    "            )\n",
    "            \n",
    "            courses.append({\n",
    "                'course_id': course_id,\n",
    "                'title': course_title,\n",
    "                'job_role': job_role,\n",
    "                'skills_required': skills_required,\n",
    "                'sub_skills_required': sub_skills_required,\n",
    "                'difficulty': difficulty,  # Add difficulty level to each course\n",
    "                'duration_hours': duration,  # Add course duration (in hours)\n",
    "                'description': course_description\n",
    "            })\n",
    "            course_id += 1\n",
    "    return pd.DataFrame(courses).drop_duplicates(subset=['job_role', 'skills_required', 'sub_skills_required'])\n",
    "\n",
    "# Generate the synthetic course data\n",
    "def generate_synthetic_data():\n",
    "    num_courses = 50  # Total number of courses\n",
    "    courses_df = generate_courses_data(num_courses)\n",
    "    \n",
    "    # Save to CSV file\n",
    "    courses_df.to_csv('courses_more_with_duration.csv', index=False) \n",
    "\n",
    "# Run data generation\n",
    "generate_synthetic_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networking courses scraped and saved to 'netacad_networking_courses.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Send a request to the specific page\n",
    "url = 'https://www.netacad.com/catalogs/learn/networking?category=course'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find all courses (You may need to inspect the HTML structure of the webpage)\n",
    "courses = soup.find_all('div', class_='course-card')\n",
    "\n",
    "# Step 4: Extract details for each course\n",
    "course_data = []\n",
    "for course in courses:\n",
    "    title = course.find('h3', class_='course-card-title').text.strip()\n",
    "    course_url = 'https://www.netacad.com' + course.find('a')['href']\n",
    "    image_url = course.find('img')['src']\n",
    "    overview = course.find('p', class_='course-card-description').text.strip()\n",
    "    \n",
    "    course_data.append({\n",
    "        'Course Title': title,\n",
    "        'URL': course_url,\n",
    "        'Image URL': image_url,\n",
    "        'Overview': overview\n",
    "    })\n",
    "\n",
    "# Step 5: Convert to DataFrame and save to CSV\n",
    "df = pd.DataFrame(course_data)\n",
    "df.to_csv('netacad_networking_courses.csv', index=False)\n",
    "\n",
    "print(\"Networking courses scraped and saved to 'netacad_networking_courses.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
